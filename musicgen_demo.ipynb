{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install soundfile\n",
    "# !pip install av\n",
    "# !pip install julius\n",
    "# !pip install torchaudio\n",
    "# !pip install omegaconf\n",
    "# !pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MusicGen\n",
    "Welcome to MusicGen's demo jupyter notebook. Here you will find a series of self-contained examples of how to use MusicGen in different settings.\n",
    "\n",
    "First, we start by initializing MusicGen, you can choose a model from the following selection:\n",
    "1. `facebook/musicgen-small` - 300M transformer decoder.\n",
    "2. `facebook/musicgen-medium` - 1.5B transformer decoder.\n",
    "3. `facebook/musicgen-melody` - 1.5B transformer decoder also supporting melody conditioning.\n",
    "4. `facebook/musicgen-large` - 3.3B transformer decoder.\n",
    "\n",
    "We will use the `facebook/musicgen-small` variant for the purpose of this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.6.0+cpu)\n",
      "    Python  3.9.13 (you have 3.9.21)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "\n",
    "USE_DIFFUSION_DECODER = False\n",
    "# Using small model, better results would be obtained with `medium` or `large`.\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us configure the generation parameters. Specifically, you can control the following:\n",
    "* `use_sampling` (bool, optional): use sampling if True, else do argmax decoding. Defaults to True.\n",
    "* `top_k` (int, optional): top_k used for sampling. Defaults to 250.\n",
    "* `top_p` (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.\n",
    "* `temperature` (float, optional): softmax temperature parameter. Defaults to 1.0.\n",
    "* `duration` (float, optional): duration of the generated waveform. Defaults to 30.0.\n",
    "* `cfg_coef` (float, optional): coefficient used for classifier free guidance. Defaults to 3.0.\n",
    "\n",
    "When left unchanged, MusicGen will revert to its default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=30\n",
    ")\n",
    "\"\"\"Set the generation parameters for MusicGen.\n",
    "\n",
    "        Args:\n",
    "            use_sampling (bool, optional): Use sampling if True, else do argmax decoding. Defaults to True.\n",
    "            top_k (int, optional): top_k used for sampling. Defaults to 250.\n",
    "            top_p (float, optional): top_p used for sampling, when set to 0 top_k is used. Defaults to 0.0.\n",
    "            temperature (float, optional): Softmax temperature parameter. Defaults to 1.0.\n",
    "            duration (float, optional): Duration of the generated waveform. Defaults to 30.0.\n",
    "            cfg_coef (float, optional): Coefficient used for classifier free guidance. Defaults to 3.0.\n",
    "            cfg_coef_beta (float, optional): beta coefficient in double classifier free guidance.\n",
    "                Should be only used for MusicGen melody if we want to push the text condition more than\n",
    "                the audio conditioning. See paragraph 4.3 in https://arxiv.org/pdf/2407.12563 to understand\n",
    "                double CFG.\n",
    "            two_step_cfg (bool, optional): If True, performs 2 forward for Classifier Free Guidance,\n",
    "                instead of batching together the two. This has some impact on how things\n",
    "                are padded but seems to have little impact in practice.\n",
    "            extend_stride: when doing extended generation (i.e. more than 30 seconds), by how much\n",
    "                should we extend the audio each time. Larger values will mean less context is\n",
    "                preserved, and shorter value will require extra computations.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can go ahead and start generating music using one of the following modes:\n",
    "* Unconditional samples using `model.generate_unconditional`\n",
    "* Music continuation using `model.generate_continuation`\n",
    "* Text-conditional samples using `model.generate`\n",
    "* Melody-conditional samples using `model.generate_with_chroma`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchaudio\n",
    "import torch\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "# def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "#                 duration=0.5, sample_rate=32000, device=\"cpu\"):\n",
    "#     \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "#     t = torch.arange(\n",
    "#         int(duration * sample_rate), device=\"cpu\", dtype=torch.float) / sample_rate\n",
    "#     wav = torch.cos(2 * math.pi * 440 * t)[None]\n",
    "#     tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "#     envelope = (tp >= 0.5).float()\n",
    "#     return wav * envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# # of the generated audio.\n",
    "# res = model.generate_continuation(\n",
    "#     get_bip_bip(0.125).expand(2, -1, -1), \n",
    "#     32000, ['Jazz jazz and only jazz', \n",
    "#             'Heartful EDM with beautiful synths and chords'], \n",
    "#     progress=True)\n",
    "# display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "# prompt_waveform, prompt_sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "# prompt_duration = 2\n",
    "# prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "# output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, progress=True, return_tokens=True)\n",
    "# display_audio(output[0], sample_rate=32000)\n",
    "# if USE_DIFFUSION_DECODER:\n",
    "#     out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "#     display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'doubao_workflow_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdoubao_workflow_test\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_music_prompt\n\u001b[0;32m      2\u001b[0m ARK_API_KEY\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9e5d6644-81ee-4360-baf1-db1816b2c344\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 示例输入\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'doubao_workflow_test'"
     ]
    }
   ],
   "source": [
    "from doubao_workflow_test import create_music_prompt\n",
    "ARK_API_KEY=\"9e5d6644-81ee-4360-baf1-db1816b2c344\"\n",
    "# 示例输入\n",
    "user_input = \"今天工作压力很大，需要放松\"\n",
    "prompt = create_music_prompt(user_input,ARK_API_KEY)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        #'80s pop track with bassy drums and synth',\n",
    "        #'90s rock song with loud guitars and heavy drums',\n",
    "        #'Progressive rock drum and bass solo',\n",
    "        #'Punk Rock song with loud drum and power guitar',\n",
    "        #'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        #'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        'drum and bass beat with intense percussions'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)\n",
    "\n",
    "exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        #'80s pop track with bassy drums and synth',\n",
    "        #'90s rock song with loud guitars and heavy drums',\n",
    "        #'Progressive rock drum and bass solo',\n",
    "        'Punk Rock song with loud drum and power guitar',\n",
    "        #'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        #'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        # 'drum and bass beat with intense percussions'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 开始计时\n",
    "start_time = time.time()\n",
    "\n",
    "# 生成输出\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'Progressive rock drum and bass solo',\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "\n",
    "# 显示音频\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "\n",
    "# 如果使用扩散解码器\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)\n",
    "\n",
    "# 结束计时\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# 输出所需时间\n",
    "print(f\"所需时间: {elapsed_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melody-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "model.set_generation_params(duration=8)\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(2, 1, 1)\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=[\n",
    "        '80s pop track with bassy drums and synth',\n",
    "        '90s rock song with loud guitars and heavy drums',\n",
    "    ],\n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "b02c911f9b3627d505ea4a19966a915ef21f28afb50dbf6b2115072d27c69103"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
